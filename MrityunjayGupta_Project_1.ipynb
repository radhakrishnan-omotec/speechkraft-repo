{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn55Rp84uwDqNKWPicEJk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/speechkraft-repo/blob/main/MrityunjayGupta_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Development of an Embedded Speech-to-Text and Emotion Recognition Device with Haptic Feedback for Individuals with Hearing Impairments"
      ],
      "metadata": {
        "id": "nUEWUG_51UJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author** : MRITYUNJAY GUPTA\n",
        "\n",
        "**Abstract** : This notebook provides a detailed stepwise methodology, incorporating literature review, hardware selection, and optimization strategies for embedded system development.\n"
      ],
      "metadata": {
        "id": "Jk4-xF-d4JbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook: Speech-to-Text and Emotion Recognition with Haptic Feedback\n"
      ],
      "metadata": {
        "id": "XG6zr9Ex2WgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "07RU7qGd2hYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Necessary Libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import time"
      ],
      "metadata": {
        "id": "kE2WiXxH2fl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Literature Review\n",
        "- Conduct a comprehensive survey of existing technologies in:\n",
        "  - Speech-to-text conversion: Explore models like DeepSpeech, Vosk, and lightweight RNNs.\n",
        "  - Emotion recognition: Study feature extraction techniques like MFCCs, pitch, and prosody.\n",
        "  - Haptic feedback mechanisms: Research vibrotactile motors and sensory substitution techniques.\n"
      ],
      "metadata": {
        "id": "SgZ9lQ5m2nNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder: Example of loading reviewed literature references\n",
        "literature = [\"Paper A on Speech Recognition\", \"Paper B on Emotion Recognition\"]\n",
        "for ref in literature:\n",
        "    print(\"Reviewed:\", ref)"
      ],
      "metadata": {
        "id": "LhoziNSc2pj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Data Collection and Preprocessing\n",
        "- **Speech Data**: Collect audio samples containing speech with labeled emotional states.\n",
        "- **Haptic Feedback Design**: Map predefined vibration patterns to specific emotions and speakers.\n",
        "\n"
      ],
      "metadata": {
        "id": "QxIEiD4q2xuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load audio file\n",
        "file_path = 'path/to/audio/file.wav'\n",
        "\n",
        "# Load audio and extract features\n",
        "audio, sr = librosa.load(file_path, sr=None)\n",
        "mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "\n",
        "# Visualize waveform and MFCCs\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.waveshow(audio, sr=sr)\n",
        "plt.title('Waveform')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfccs, x_axis='time', sr=sr)\n",
        "plt.colorbar()\n",
        "plt.title('MFCCs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1HfPQH-j2zoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Feature Extraction\n",
        "- Extract **MFCCs**, **pitch**, and **prosody** for emotion classification.\n"
      ],
      "metadata": {
        "id": "1lWkibnK29LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract pitch and prosody\n",
        "pitch, voiced_flag, _ = librosa.pyin(audio, fmin=50, fmax=300, sr=sr)\n",
        "energy = librosa.feature.rms(y=audio)\n",
        "\n",
        "# Handle NaN values in pitch\n",
        "pitch = np.nan_to_num(pitch)\n",
        "\n",
        "# Combine features into a single array\n",
        "features = np.concatenate((mfccs.mean(axis=1), [np.mean(pitch)], [np.mean(energy)]))"
      ],
      "metadata": {
        "id": "PAIC-hvG2_Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Emotion Classification Model\n",
        "- Train a lightweight model (e.g., **SVM** or **CNN**) for classifying emotions.\n"
      ],
      "metadata": {
        "id": "I61IprNo3BTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (example placeholder)\n",
        "data = np.load('path/to/emotion_dataset.npz')\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "clf = SVC(kernel='linear', probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "zLUL532C3H-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Speech-to-Text Conversion\n",
        "- Use pre-trained lightweight models like **DeepSpeech** or **RNNs** optimized for embedded systems.\n"
      ],
      "metadata": {
        "id": "9xvsmaHa3J_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "# Initialize Vosk model\n",
        "model = Model(\"path/to/vosk-model\")\n",
        "recognizer = KaldiRecognizer(model, sr)\n",
        "\n",
        "# Convert audio to text\n",
        "if recognizer.AcceptWaveform(audio):\n",
        "    result = recognizer.Result()\n",
        "    print(\"Speech-to-Text Result:\", result)"
      ],
      "metadata": {
        "id": "lqFI51-a3QUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Haptic Feedback Design\n",
        "- Develop distinct vibration patterns for emotions and speaker identity using **actuators**.\n",
        "\n"
      ],
      "metadata": {
        "id": "NzqJ4Lo23SbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define haptic patterns\n",
        "def generate_haptic_feedback(emotion):\n",
        "    patterns = {\n",
        "        'happy': [0.5, 0.2, 0.5],  # vibration ON-OFF-ON pattern\n",
        "        'sad': [1.0, 0.5],         # longer vibration\n",
        "        'angry': [0.2, 0.2, 0.2]   # short bursts\n",
        "    }\n",
        "    return patterns.get(emotion, [0.5])  # Default pattern\n",
        "\n",
        "# Trigger actuator (example function)\n",
        "def trigger_haptic(pattern):\n",
        "    for duration in pattern:\n",
        "        print(f\"Vibrating for {duration} seconds\")\n",
        "        time.sleep(duration)\n",
        "\n",
        "# Example usage\n",
        "emotion = 'happy'\n",
        "pattern = generate_haptic_feedback(emotion)\n",
        "trigger_haptic(pattern)"
      ],
      "metadata": {
        "id": "fjLf5-6h3ZdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Embedded Hardware Platform Selection\n",
        "- Select a Raspberry Pi (Rpi) or similar embedded system with adequate computational power.\n",
        "- Develop lightweight models optimized for embedded systems using TensorFlow Lite or ONNX.\n",
        "\n"
      ],
      "metadata": {
        "id": "LG-Qlb-S3b0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy TensorFlow Lite model\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "# Load TFLite model\n",
        "interpreter = tflite.Interpreter(model_path=\"path/to/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Predict emotion\n",
        "interpreter.set_tensor(input_details[0]['index'], features.reshape(1, -1))\n",
        "interpreter.invoke()\n",
        "emotion_prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Predicted Emotion:\", emotion_prediction)"
      ],
      "metadata": {
        "id": "91ryP5yJ3kj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 9 [OPTIONAL] : Power Management Strategies\n",
        "- Implement sleep modes and power-saving techniques to optimize battery usage.\n",
        "\n"
      ],
      "metadata": {
        "id": "-wr05z-b3mWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example power-saving function\n",
        "def manage_power(mode):\n",
        "    if mode == 'low-power':\n",
        "        print(\"Activating low-power mode...\")\n",
        "    elif mode == 'performance':\n",
        "        print(\"Activating performance mode...\")\n",
        "\n",
        "# Usage\n",
        "manage_power('low-power')"
      ],
      "metadata": {
        "id": "vUGal9iq34Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Testing and Validation\n",
        "- Evaluate the integrated system with test cases covering:\n",
        "  - Speech-to-text accuracy\n",
        "  - Emotion recognition performance\n",
        "  - Usability of haptic feedback"
      ],
      "metadata": {
        "id": "5rktyJjV36T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test speech-to-text\n",
        "text_result = recognizer.Result()\n",
        "print(\"Transcribed Text:\", text_result)\n",
        "\n",
        "# Test emotion classification\n",
        "emotion_result = clf.predict(features.reshape(1, -1))\n",
        "print(\"Detected Emotion:\", emotion_result)\n",
        "\n",
        "# Validate haptic feedback\n",
        "trigger_haptic(generate_haptic_feedback(emotion_result[0]))"
      ],
      "metadata": {
        "id": "w5Y-VIla4BMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "----"
      ],
      "metadata": {
        "id": "5jjNf3l_2tD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Notebook: Speech-to-Text and Emotion Recognition with Haptic Feedback\n",
        "\n",
        "# Step 1: Import Necessary Libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "```\n",
        "\n",
        "# Step 2: Literature Review\n",
        "- Conduct a comprehensive survey of existing technologies in:\n",
        "  - Speech-to-text conversion: Explore models like DeepSpeech, Vosk, and lightweight RNNs.\n",
        "  - Emotion recognition: Study feature extraction techniques like MFCCs, pitch, and prosody.\n",
        "  - Haptic feedback mechanisms: Research vibrotactile motors and sensory substitution techniques.\n",
        "\n",
        "```python\n",
        "# Placeholder: Example of loading reviewed literature references\n",
        "literature = [\"Paper A on Speech Recognition\", \"Paper B on Emotion Recognition\"]\n",
        "for ref in literature:\n",
        "    print(\"Reviewed:\", ref)\n",
        "```\n",
        "\n",
        "# Step 3: Data Collection and Preprocessing\n",
        "- **Speech Data**: Collect audio samples containing speech with labeled emotional states.\n",
        "- **Haptic Feedback Design**: Map predefined vibration patterns to specific emotions and speakers.\n",
        "\n",
        "```python\n",
        "# Load audio file\n",
        "file_path = 'path/to/audio/file.wav'\n",
        "\n",
        "# Load audio and extract features\n",
        "audio, sr = librosa.load(file_path, sr=None)\n",
        "mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "\n",
        "# Visualize waveform and MFCCs\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.waveshow(audio, sr=sr)\n",
        "plt.title('Waveform')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfccs, x_axis='time', sr=sr)\n",
        "plt.colorbar()\n",
        "plt.title('MFCCs')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "# Step 4: Feature Extraction\n",
        "- Extract **MFCCs**, **pitch**, and **prosody** for emotion classification.\n",
        "\n",
        "```python\n",
        "# Extract pitch and prosody\n",
        "pitch, voiced_flag, _ = librosa.pyin(audio, fmin=50, fmax=300, sr=sr)\n",
        "energy = librosa.feature.rms(y=audio)\n",
        "\n",
        "# Handle NaN values in pitch\n",
        "pitch = np.nan_to_num(pitch)\n",
        "\n",
        "# Combine features into a single array\n",
        "features = np.concatenate((mfccs.mean(axis=1), [np.mean(pitch)], [np.mean(energy)]))\n",
        "```\n",
        "\n",
        "# Step 5: Emotion Classification Model\n",
        "- Train a lightweight model (e.g., **SVM** or **CNN**) for classifying emotions.\n",
        "\n",
        "```python\n",
        "# Load dataset (example placeholder)\n",
        "data = np.load('path/to/emotion_dataset.npz')\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "clf = SVC(kernel='linear', probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "```\n",
        "\n",
        "# Step 6: Speech-to-Text Conversion\n",
        "- Use pre-trained lightweight models like **DeepSpeech** or **RNNs** optimized for embedded systems.\n",
        "\n",
        "```python\n",
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "# Initialize Vosk model\n",
        "model = Model(\"path/to/vosk-model\")\n",
        "recognizer = KaldiRecognizer(model, sr)\n",
        "\n",
        "# Convert audio to text\n",
        "if recognizer.AcceptWaveform(audio):\n",
        "    result = recognizer.Result()\n",
        "    print(\"Speech-to-Text Result:\", result)\n",
        "```\n",
        "\n",
        "# Step 7: Haptic Feedback Design\n",
        "- Develop distinct vibration patterns for emotions and speaker identity using **actuators**.\n",
        "\n",
        "```python\n",
        "# Define haptic patterns\n",
        "def generate_haptic_feedback(emotion):\n",
        "    patterns = {\n",
        "        'happy': [0.5, 0.2, 0.5],  # vibration ON-OFF-ON pattern\n",
        "        'sad': [1.0, 0.5],         # longer vibration\n",
        "        'angry': [0.2, 0.2, 0.2]   # short bursts\n",
        "    }\n",
        "    return patterns.get(emotion, [0.5])  # Default pattern\n",
        "\n",
        "# Trigger actuator (example function)\n",
        "def trigger_haptic(pattern):\n",
        "    for duration in pattern:\n",
        "        print(f\"Vibrating for {duration} seconds\")\n",
        "        time.sleep(duration)\n",
        "\n",
        "# Example usage\n",
        "emotion = 'happy'\n",
        "pattern = generate_haptic_feedback(emotion)\n",
        "trigger_haptic(pattern)\n",
        "```\n",
        "\n",
        "# Step 8: Embedded Hardware Platform Selection\n",
        "- Select a Raspberry Pi (Rpi) or similar embedded system with adequate computational power.\n",
        "- Develop lightweight models optimized for embedded systems using TensorFlow Lite or ONNX.\n",
        "\n",
        "```python\n",
        "# Deploy TensorFlow Lite model\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "# Load TFLite model\n",
        "interpreter = tflite.Interpreter(model_path=\"path/to/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Predict emotion\n",
        "interpreter.set_tensor(input_details[0]['index'], features.reshape(1, -1))\n",
        "interpreter.invoke()\n",
        "emotion_prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Predicted Emotion:\", emotion_prediction)\n",
        "```\n",
        "\n",
        "# Step 9: Power Management Strategies\n",
        "- Implement sleep modes and power-saving techniques to optimize battery usage.\n",
        "\n",
        "```python\n",
        "# Example power-saving function\n",
        "def manage_power(mode):\n",
        "    if mode == 'low-power':\n",
        "        print(\"Activating low-power mode...\")\n",
        "    elif mode == 'performance':\n",
        "        print(\"Activating performance mode...\")\n",
        "\n",
        "# Usage\n",
        "manage_power('low-power')\n",
        "```\n",
        "\n",
        "# Step 10: Testing and Validation\n",
        "- Evaluate the integrated system with test cases covering:\n",
        "  - Speech-to-text accuracy\n",
        "  - Emotion recognition performance\n",
        "  - Usability of haptic feedback\n",
        "\n",
        "```python\n",
        "# Test speech-to-text\n",
        "text_result = recognizer.Result()\n",
        "print(\"Transcribed Text:\", text_result)\n",
        "\n",
        "# Test emotion classification\n",
        "emotion_result = clf.predict(features.reshape(1, -1))\n",
        "print(\"Detected Emotion:\", emotion_result)\n",
        "\n",
        "# Validate haptic feedback\n",
        "trigger_haptic(generate_haptic_feedback(emotion_result[0]))\n",
        "```\n",
        "\n",
        "This enhanced Python notebook provides a detailed stepwise methodology, incorporating literature review, hardware selection, and optimization strategies for embedded system development.\n"
      ],
      "metadata": {
        "id": "nqdKBN3b2XSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OLD"
      ],
      "metadata": {
        "id": "pidGe3Yk2Sur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1_METAg1Mju"
      },
      "outputs": [],
      "source": [
        "### Notebook: Speech-to-Text and Emotion Recognition with Haptic Feedback\n",
        "\n",
        "# Step 1: Import Necessary Libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "```\n",
        "\n",
        "# Step 2: Data Collection and Preprocessing\n",
        "- **Speech Data**: Collect audio samples containing speech with labeled emotional states.\n",
        "- **Haptic Feedback Design**: Map predefined vibration patterns to specific emotions and speakers.\n",
        "\n",
        "```python\n",
        "# Load audio file\n",
        "file_path = 'path/to/audio/file.wav'\n",
        "\n",
        "# Load audio and extract features\n",
        "audio, sr = librosa.load(file_path, sr=None)\n",
        "mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "```\n",
        "\n",
        "# Step 3: Feature Extraction\n",
        "- Extract **MFCCs**, **pitch**, and **prosody** for emotion classification.\n",
        "\n",
        "```python\n",
        "# Extract pitch and prosody\n",
        "pitch = librosa.pyin(audio, fmin=50, fmax=300, sr=sr)[0]\n",
        "energy = librosa.feature.rms(y=audio)\n",
        "\n",
        "# Combine features into a single array\n",
        "features = np.concatenate((mfccs.mean(axis=1), [np.mean(pitch)], [np.mean(energy)]))\n",
        "```\n",
        "\n",
        "# Step 4: Emotion Classification Model\n",
        "- Train a lightweight model (e.g., **SVM** or **CNN**) for classifying emotions.\n",
        "\n",
        "```python\n",
        "# Load dataset (example placeholder)\n",
        "data = np.load('path/to/emotion_dataset.npz')\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "clf = SVC(kernel='linear', probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "```\n",
        "\n",
        "# Step 5: Speech-to-Text Conversion\n",
        "- Use pre-trained lightweight models like **DeepSpeech** or **RNNs** optimized for embedded systems.\n",
        "\n",
        "```python\n",
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "# Initialize Vosk model\n",
        "model = Model(\"path/to/vosk-model\")\n",
        "recognizer = KaldiRecognizer(model, sr)\n",
        "\n",
        "# Convert audio to text\n",
        "if recognizer.AcceptWaveform(audio):\n",
        "    result = recognizer.Result()\n",
        "    print(result)\n",
        "```\n",
        "\n",
        "# Step 6: Haptic Feedback Design\n",
        "- Develop distinct vibration patterns for emotions and speaker identity using **actuators**.\n",
        "\n",
        "```python\n",
        "# Define haptic patterns\n",
        "def generate_haptic_feedback(emotion):\n",
        "    patterns = {\n",
        "        'happy': [0.5, 0.2, 0.5],  # vibration ON-OFF-ON pattern\n",
        "        'sad': [1.0, 0.5],         # longer vibration\n",
        "        'angry': [0.2, 0.2, 0.2]   # short bursts\n",
        "    }\n",
        "    return patterns.get(emotion, [0.5])  # Default pattern\n",
        "\n",
        "# Trigger actuator (example function)\n",
        "def trigger_haptic(pattern):\n",
        "    for duration in pattern:\n",
        "        print(f\"Vibrating for {duration} seconds\")\n",
        "        time.sleep(duration)\n",
        "\n",
        "# Example usage\n",
        "emotion = 'happy'\n",
        "pattern = generate_haptic_feedback(emotion)\n",
        "trigger_haptic(pattern)\n",
        "```\n",
        "\n",
        "# Step 7: Integration on Embedded Platform\n",
        "- Use Raspberry Pi or similar microcontroller for integration.\n",
        "- Optimize models and manage power consumption for wearability.\n",
        "\n",
        "```python\n",
        "# Deploy TensorFlow Lite model\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "# Load TFLite model\n",
        "interpreter = tflite.Interpreter(model_path=\"path/to/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Predict emotion\n",
        "interpreter.set_tensor(input_details[0]['index'], features.reshape(1, -1))\n",
        "interpreter.invoke()\n",
        "emotion_prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Predicted Emotion:\", emotion_prediction)\n",
        "```\n",
        "\n",
        "# Step 8: Testing and Validation\n",
        "- Evaluate the integrated system with test cases covering speech-to-text accuracy, emotion recognition performance, and usability of haptic feedback.\n",
        "\n",
        "```python\n",
        "# Test speech-to-text\n",
        "text_result = recognizer.Result()\n",
        "print(\"Transcribed Text:\", text_result)\n",
        "\n",
        "# Test emotion classification\n",
        "emotion_result = clf.predict(features.reshape(1, -1))\n",
        "print(\"Detected Emotion:\", emotion_result)\n",
        "\n",
        "# Validate haptic feedback\n",
        "trigger_haptic(generate_haptic_feedback(emotion_result[0]))\n",
        "```\n",
        "\n",
        "This Python notebook represents a modular stepwise approach, emphasizing modularity for easy debugging and optimization during embedded implementation.\n"
      ]
    }
  ]
}